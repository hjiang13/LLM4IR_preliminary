#!/usr/bin/env python3
"""
ä¼˜åŒ–ç ”ç©¶ - æµ‹è¯•ä¸åŒMLIRä¼˜åŒ–ä¼ é€’çš„æ•ˆæœ
"""

import os
import json
import subprocess
import time
import statistics
from pathlib import Path
from datetime import datetime

class OptimizationStudy:
    """ä¼˜åŒ–ç ”ç©¶ç±»"""
    
    def __init__(self):
        self.results = {
            "study_info": {
                "timestamp": datetime.now().isoformat(),
                "study_type": "optimization_effectiveness",
                "total_operations": 0,
                "analyzed_operations": 0
            },
            "pass_combinations": {},
            "operation_results": {},
            "optimization_metrics": {},
            "recommendations": {}
        }
        
        # å®šä¹‰ä¸åŒçš„ä¼˜åŒ–ä¼ é€’ç»„åˆ
        self.pass_combinations = {
            "minimal": [
                "convert-linalg-to-loops"
            ],
            "basic": [
                "convert-linalg-to-loops",
                "lower-affine",
                "convert-scf-to-cf"
            ],
            "intermediate": [
                "convert-linalg-to-loops",
                "lower-affine", 
                "convert-scf-to-cf",
                "convert-math-to-llvm",
                "convert-arith-to-llvm"
            ],
            "advanced": [
                "convert-linalg-to-loops",
                "lower-affine",
                "convert-scf-to-cf", 
                "convert-math-to-llvm",
                "convert-arith-to-llvm",
                "convert-func-to-llvm"
            ],
            "full": [
                "one-shot-bufferize=bufferize-function-boundaries",
                "convert-linalg-to-loops",
                "lower-affine",
                "convert-scf-to-cf",
                "convert-math-to-llvm", 
                "convert-arith-to-llvm",
                "convert-func-to-llvm",
                "reconcile-unrealized-casts"
            ]
        }
    
    def test_optimization_passes(self, mlir_file, pass_combination, iterations=5):
        """æµ‹è¯•ç‰¹å®šä¼˜åŒ–ä¼ é€’ç»„åˆ"""
        times = []
        success_count = 0
        output_sizes = []
        
        for _ in range(iterations):
            start_time = time.time()
            try:
                cmd = ["mlir-opt", mlir_file] + [f"--{pass_name}" for pass_name in pass_combination]
                result = subprocess.run(
                    cmd,
                    capture_output=True,
                    text=True,
                    timeout=120
                )
                end_time = time.time()
                
                if result.returncode == 0:
                    times.append(end_time - start_time)
                    success_count += 1
                    # ä¼°ç®—è¾“å‡ºå¤§å°ï¼ˆè¡Œæ•°ï¼‰
                    output_sizes.append(len(result.stdout.split('\n')))
            except (subprocess.TimeoutExpired, Exception):
                continue
        
        if times:
            return {
                "success_rate": success_count / iterations,
                "mean_time": statistics.mean(times),
                "median_time": statistics.median(times),
                "min_time": min(times),
                "max_time": max(times),
                "std_time": statistics.stdev(times) if len(times) > 1 else 0,
                "mean_output_size": statistics.mean(output_sizes) if output_sizes else 0,
                "samples": len(times)
            }
        return None
    
    def analyze_operation_optimization(self, operation_name, mlir_file):
        """åˆ†æå•ä¸ªæ“ä½œçš„ä¼˜åŒ–æ•ˆæœ"""
        print(f"  åˆ†ææ“ä½œ: {operation_name}")
        
        op_result = {
            "operation": operation_name,
            "mlir_file": mlir_file,
            "pass_results": {},
            "best_pass": None,
            "optimization_effectiveness": {}
        }
        
        # æµ‹è¯•æ‰€æœ‰ä¼ é€’ç»„åˆ
        for pass_name, passes in self.pass_combinations.items():
            result = self.test_optimization_passes(mlir_file, passes, 3)
            if result:
                op_result["pass_results"][pass_name] = result
        
        # æ‰¾å‡ºæœ€ä½³ä¼ é€’ç»„åˆ
        if op_result["pass_results"]:
            best_pass = min(op_result["pass_results"].items(), 
                          key=lambda x: x[1]["mean_time"])
            op_result["best_pass"] = best_pass[0]
            
            # è®¡ç®—ä¼˜åŒ–æ•ˆæœ
            if "minimal" in op_result["pass_results"] and "full" in op_result["pass_results"]:
                minimal_time = op_result["pass_results"]["minimal"]["mean_time"]
                full_time = op_result["pass_results"]["full"]["mean_time"]
                op_result["optimization_effectiveness"] = {
                    "time_improvement": (minimal_time - full_time) / minimal_time * 100,
                    "output_size_change": op_result["pass_results"]["full"]["mean_output_size"] - 
                                        op_result["pass_results"]["minimal"]["mean_output_size"]
                }
        
        return op_result
    
    def run_optimization_study(self, sample_size=10):
        """è¿è¡Œä¼˜åŒ–ç ”ç©¶"""
        print("ğŸ”¬ å¼€å§‹ä¼˜åŒ–ç ”ç©¶...")
        
        # è·å–æ“ä½œåˆ—è¡¨
        single_ops = []
        if os.path.exists("out/single"):
            single_ops = [d for d in os.listdir("out/single") 
                         if os.path.isdir(os.path.join("out/single", d))]
        
        # éšæœºé‡‡æ ·
        import random
        sample_ops = random.sample(single_ops, min(sample_size, len(single_ops)))
        
        for op in sample_ops:
            mlir_file = f"out/single/{op}/{op}_N1_H8_W8_C8.mlir"
            if os.path.exists(mlir_file):
                op_result = self.analyze_operation_optimization(op, mlir_file)
                self.results["operation_results"][op] = op_result
                self.results["study_info"]["analyzed_operations"] += 1
        
        self.results["study_info"]["total_operations"] = len(sample_ops)
    
    def analyze_pass_effectiveness(self):
        """åˆ†æä¼ é€’æ•ˆæœ"""
        print("ğŸ“Š åˆ†æä¼ é€’æ•ˆæœ...")
        
        pass_stats = {}
        
        for pass_name in self.pass_combinations.keys():
            times = []
            success_rates = []
            output_sizes = []
            
            for op, result in self.results["operation_results"].items():
                if pass_name in result["pass_results"]:
                    pass_result = result["pass_results"][pass_name]
                    times.append(pass_result["mean_time"])
                    success_rates.append(pass_result["success_rate"])
                    output_sizes.append(pass_result["mean_output_size"])
            
            if times:
                pass_stats[pass_name] = {
                    "mean_time": statistics.mean(times),
                    "median_time": statistics.median(times),
                    "mean_success_rate": statistics.mean(success_rates),
                    "mean_output_size": statistics.mean(output_sizes),
                    "operations_tested": len(times)
                }
        
        self.results["pass_combinations"] = pass_stats
    
    def generate_optimization_metrics(self):
        """ç”Ÿæˆä¼˜åŒ–æŒ‡æ ‡"""
        print("ğŸ“ˆ ç”Ÿæˆä¼˜åŒ–æŒ‡æ ‡...")
        
        metrics = {
            "time_efficiency": {},
            "success_reliability": {},
            "output_quality": {},
            "overall_ranking": []
        }
        
        # æ—¶é—´æ•ˆç‡æ’å
        time_ranking = []
        for pass_name, stats in self.results["pass_combinations"].items():
            time_ranking.append((pass_name, stats["mean_time"]))
        time_ranking.sort(key=lambda x: x[1])
        metrics["time_efficiency"] = {rank[0]: i+1 for i, rank in enumerate(time_ranking)}
        
        # æˆåŠŸç‡æ’å
        success_ranking = []
        for pass_name, stats in self.results["pass_combinations"].items():
            success_ranking.append((pass_name, stats["mean_success_rate"]))
        success_ranking.sort(key=lambda x: x[1], reverse=True)
        metrics["success_reliability"] = {rank[0]: i+1 for i, rank in enumerate(success_ranking)}
        
        # ç»¼åˆæ’å
        overall_scores = {}
        for pass_name in self.pass_combinations.keys():
            time_rank = metrics["time_efficiency"].get(pass_name, 999)
            success_rank = metrics["success_reliability"].get(pass_name, 999)
            overall_scores[pass_name] = time_rank + success_rank
        
        overall_ranking = sorted(overall_scores.items(), key=lambda x: x[1])
        metrics["overall_ranking"] = [rank[0] for rank in overall_ranking]
        
        self.results["optimization_metrics"] = metrics
    
    def generate_recommendations(self):
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        print("ğŸ’¡ ç”Ÿæˆä¼˜åŒ–å»ºè®®...")
        
        recommendations = {
            "best_overall": self.results["optimization_metrics"]["overall_ranking"][0],
            "fastest": min(self.results["pass_combinations"].items(), key=lambda x: x[1]["mean_time"])[0],
            "most_reliable": max(self.results["pass_combinations"].items(), key=lambda x: x[1]["mean_success_rate"])[0],
            "use_cases": {
                "development": "basic",
                "production": "advanced", 
                "maximum_optimization": "full",
                "quick_testing": "minimal"
            },
            "operation_specific": {}
        }
        
        # æ“ä½œç‰¹å®šå»ºè®®
        for op, result in self.results["operation_results"].items():
            if result["best_pass"]:
                recommendations["operation_specific"][op] = result["best_pass"]
        
        self.results["recommendations"] = recommendations
    
    def generate_report(self):
        """ç”Ÿæˆä¼˜åŒ–ç ”ç©¶æŠ¥å‘Š"""
        report_file = "experiments/results/optimization_study_report.json"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(report_file), exist_ok=True)
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        self.generate_markdown_report()
        
        print(f"âœ… ä¼˜åŒ–ç ”ç©¶æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
    
    def generate_markdown_report(self):
        """ç”ŸæˆMarkdownæŠ¥å‘Š"""
        report_file = "experiments/results/optimization_study_report.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# ä¼˜åŒ–ç ”ç©¶æŠ¥å‘Š\n\n")
            f.write(f"**ç ”ç©¶æ—¶é—´**: {self.results['study_info']['timestamp']}\n")
            f.write(f"**ç ”ç©¶ç±»å‹**: {self.results['study_info']['study_type']}\n")
            f.write(f"**åˆ†ææ“ä½œæ•°**: {self.results['study_info']['analyzed_operations']}\n\n")
            
            # ä¼ é€’ç»„åˆæ•ˆæœ
            f.write("## ğŸ”§ ä¼ é€’ç»„åˆæ•ˆæœ\n\n")
            f.write("| ç»„åˆ | å¹³å‡æ—¶é—´(ms) | æˆåŠŸç‡ | è¾“å‡ºå¤§å° | æ’å |\n")
            f.write("|------|-------------|--------|----------|------|\n")
            
            for pass_name, stats in self.results["pass_combinations"].items():
                time_ms = stats["mean_time"] * 1000
                success_rate = stats["mean_success_rate"] * 100
                output_size = stats["mean_output_size"]
                overall_rank = self.results["optimization_metrics"]["overall_ranking"].index(pass_name) + 1
                
                f.write(f"| {pass_name} | {time_ms:.2f} | {success_rate:.1f}% | {output_size:.0f} | {overall_rank} |\n")
            
            # ä¼˜åŒ–å»ºè®®
            f.write("\n## ğŸ’¡ ä¼˜åŒ–å»ºè®®\n\n")
            recommendations = self.results["recommendations"]
            
            f.write("### æ€»ä½“å»ºè®®\n\n")
            f.write(f"- **æœ€ä½³ç»¼åˆ**: {recommendations['best_overall']}\n")
            f.write(f"- **æœ€å¿«é€Ÿåº¦**: {recommendations['fastest']}\n")
            f.write(f"- **æœ€é«˜å¯é æ€§**: {recommendations['most_reliable']}\n\n")
            
            f.write("### ä½¿ç”¨åœºæ™¯å»ºè®®\n\n")
            for use_case, pass_name in recommendations["use_cases"].items():
                f.write(f"- **{use_case}**: {pass_name}\n")
            
            # æ“ä½œç‰¹å®šå»ºè®®
            f.write("\n### æ“ä½œç‰¹å®šå»ºè®®\n\n")
            f.write("| æ“ä½œ | æ¨èä¼ é€’ç»„åˆ |\n")
            f.write("|------|-------------|\n")
            
            for op, best_pass in recommendations["operation_specific"].items():
                f.write(f"| {op} | {best_pass} |\n")
            
            # è¯¦ç»†åˆ†æ
            f.write("\n## ğŸ“Š è¯¦ç»†åˆ†æ\n\n")
            f.write("### æ—¶é—´æ•ˆç‡æ’å\n\n")
            for i, pass_name in enumerate(self.results["optimization_metrics"]["overall_ranking"], 1):
                f.write(f"{i}. **{pass_name}**\n")
            
            f.write("\n### æ“ä½œä¼˜åŒ–æ•ˆæœ\n\n")
            f.write("| æ“ä½œ | æœ€ä½³ä¼ é€’ | æ—¶é—´æ”¹è¿› | è¾“å‡ºå˜åŒ– |\n")
            f.write("|------|----------|----------|----------|\n")
            
            for op, result in self.results["operation_results"].items():
                best_pass = result["best_pass"] or "N/A"
                time_improvement = result["optimization_effectiveness"].get("time_improvement", 0)
                output_change = result["optimization_effectiveness"].get("output_size_change", 0)
                
                f.write(f"| {op} | {best_pass} | {time_improvement:.1f}% | {output_change:+.0f} |\n")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹ä¼˜åŒ–ç ”ç©¶...")
    
    study = OptimizationStudy()
    
    # è¿è¡Œä¼˜åŒ–ç ”ç©¶
    study.run_optimization_study(sample_size=12)
    
    # åˆ†æä¼ é€’æ•ˆæœ
    study.analyze_pass_effectiveness()
    
    # ç”Ÿæˆä¼˜åŒ–æŒ‡æ ‡
    study.generate_optimization_metrics()
    
    # ç”Ÿæˆå»ºè®®
    study.generate_recommendations()
    
    # ç”ŸæˆæŠ¥å‘Š
    study.generate_report()
    
    print("âœ… ä¼˜åŒ–ç ”ç©¶å®Œæˆï¼")
    print(f"ğŸ“Š åˆ†ææ“ä½œæ•°: {study.results['study_info']['analyzed_operations']}")

if __name__ == "__main__":
    main()
