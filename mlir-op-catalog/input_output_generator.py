#!/usr/bin/env python3
"""
è¾“å…¥-è¾“å‡º-IRæ•°æ®é›†ç”Ÿæˆå™¨
ä¸ºæ‰€æœ‰IRæ–‡ä»¶ç”Ÿæˆè¾“å…¥æ•°æ®ï¼Œè¿è¡Œå®ƒä»¬ï¼Œå¹¶è®°å½•è¾“å‡º
"""

import os
import json
import numpy as np
import subprocess
import tempfile
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Any

class InputOutputGenerator:
    """è¾“å…¥è¾“å‡ºç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.base_dir = Path(".")
        self.output_dir = self.base_dir / "io_dataset"
        self.temp_dir = Path("/tmp/mlir_io_test")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(exist_ok=True)
        self.temp_dir.mkdir(exist_ok=True)
        
        # æ•°æ®é›†ç»“æ„
        self.dataset = {
            "metadata": {
                "generated_at": datetime.now().isoformat(),
                "total_operations": 0,
                "single_operations": 0,
                "pair_operations": 0,
                "successful_tests": 0,
                "failed_tests": 0
            },
            "single_operations": {},
            "pair_operations": {},
            "statistics": {}
        }
        
        # è¾“å…¥æ•°æ®é…ç½®
        self.input_configs = {
            "tensor_1x8x8x8": {
                "shape": [1, 8, 8, 8],
                "dtype": "float32",
                "range": [-1.0, 1.0]
            },
            "tensor_16x16": {
                "shape": [16, 16],
                "dtype": "float32", 
                "range": [-1.0, 1.0]
            },
            "tensor_32x32": {
                "shape": [32, 32],
                "dtype": "float32",
                "range": [-1.0, 1.0]
            },
            "conv_input": {
                "shape": [1, 8, 8, 8],
                "dtype": "float32",
                "range": [0.0, 1.0]
            },
            "conv_kernel": {
                "shape": [3, 3, 8, 8],
                "dtype": "float32",
                "range": [-0.5, 0.5]
            }
        }
    
    def generate_input_data(self, tensor_config: Dict) -> np.ndarray:
        """ç”Ÿæˆè¾“å…¥æ•°æ®"""
        shape = tensor_config["shape"]
        dtype = tensor_config["dtype"]
        min_val, max_val = tensor_config["range"]
        
        # ç”Ÿæˆéšæœºæ•°æ®
        data = np.random.uniform(min_val, max_val, shape).astype(dtype)
        return data
    
    def save_tensor_to_file(self, tensor: np.ndarray, filename: str) -> str:
        """å°†å¼ é‡ä¿å­˜åˆ°æ–‡ä»¶"""
        filepath = self.temp_dir / filename
        np.save(filepath, tensor)
        return str(filepath)
    
    def load_tensor_from_file(self, filepath: str) -> np.ndarray:
        """ä»æ–‡ä»¶åŠ è½½å¼ é‡"""
        return np.load(filepath)
    
    def create_c_wrapper(self, operation_name: str, input_shapes: List[List[int]], 
                        output_shape: List[int], llvm_file: str) -> str:
        """åˆ›å»ºCåŒ…è£…å™¨æ¥è°ƒç”¨LLVM IRå‡½æ•°"""
        
        # ç”Ÿæˆå‡½æ•°ç­¾å
        input_params = []
        for i, shape in enumerate(input_shapes):
            total_elements = np.prod(shape)
            input_params.append(f"float* input_{i}")
            input_params.append(f"int64_t* input_{i}_shape")
            input_params.append(f"int64_t* input_{i}_strides")
        
        output_params = [
            "float* output",
            "int64_t* output_shape", 
            "int64_t* output_strides"
        ]
        
        all_params = input_params + output_params
        function_signature = f"void mlir_{operation_name}({', '.join(all_params)})"
        
        c_code = f"""
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <math.h>

// å¤–éƒ¨å‡½æ•°å£°æ˜
extern void main({', '.join(all_params)});

{function_signature} {{
    // è°ƒç”¨MLIRç”Ÿæˆçš„å‡½æ•°
    main({', '.join([f"input_{i}, input_{i}_shape, input_{i}_strides" for i in range(len(input_shapes))])}, 
         output, output_shape, output_strides);
}}

int main() {{
    // è¿™é‡Œå¯ä»¥æ·»åŠ æµ‹è¯•ä»£ç 
    return 0;
}}
"""
        
        return c_code
    
    def compile_and_run_llvm(self, llvm_file: str, c_wrapper: str, 
                           input_data: List[np.ndarray], operation_name: str) -> Tuple[bool, Any, str]:
        """ç¼–è¯‘å¹¶è¿è¡ŒLLVM IR"""
        try:
            # ä¿å­˜CåŒ…è£…å™¨
            c_file = self.temp_dir / f"{operation_name}_wrapper.c"
            with open(c_file, 'w') as f:
                f.write(c_wrapper)
            
            # ç¼–è¯‘LLVM IRåˆ°ç›®æ ‡æ–‡ä»¶
            bc_file = self.temp_dir / f"{operation_name}.bc"
            result = subprocess.run(
                ["llvm-as", llvm_file, "-o", str(bc_file)],
                capture_output=True, text=True, timeout=30
            )
            if result.returncode != 0:
                return False, None, f"LLVM assembly failed: {result.stderr}"
            
            # ç¼–è¯‘Cä»£ç å’ŒLLVM IR
            obj_file = self.temp_dir / f"{operation_name}.o"
            result = subprocess.run(
                ["llc", str(bc_file), "-o", str(obj_file)],
                capture_output=True, text=True, timeout=30
            )
            if result.returncode != 0:
                return False, None, f"LLC compilation failed: {result.stderr}"
            
            # ç¼–è¯‘CåŒ…è£…å™¨
            wrapper_obj = self.temp_dir / f"{operation_name}_wrapper.o"
            result = subprocess.run(
                ["gcc", "-c", str(c_file), "-o", str(wrapper_obj)],
                capture_output=True, text=True, timeout=30
            )
            if result.returncode != 0:
                return False, None, f"C compilation failed: {result.stderr}"
            
            # é“¾æ¥ç”Ÿæˆå¯æ‰§è¡Œæ–‡ä»¶
            exe_file = self.temp_dir / f"{operation_name}_test"
            result = subprocess.run(
                ["gcc", str(obj_file), str(wrapper_obj), "-o", str(exe_file)],
                capture_output=True, text=True, timeout=30
            )
            if result.returncode != 0:
                return False, None, f"Linking failed: {result.stderr}"
            
            # å‡†å¤‡è¾“å…¥æ•°æ®æ–‡ä»¶
            input_files = []
            for i, data in enumerate(input_data):
                input_file = self.temp_dir / f"{operation_name}_input_{i}.npy"
                np.save(input_file, data)
                input_files.append(str(input_file))
            
            # è¿è¡Œç¨‹åº
            result = subprocess.run(
                [str(exe_file)] + input_files,
                capture_output=True, text=True, timeout=60
            )
            
            if result.returncode != 0:
                return False, None, f"Execution failed: {result.stderr}"
            
            # è¯»å–è¾“å‡ºæ•°æ®
            output_file = self.temp_dir / f"{operation_name}_output.npy"
            if output_file.exists():
                output_data = np.load(output_file)
                return True, output_data, None
            else:
                return False, None, "No output file generated"
                
        except subprocess.TimeoutExpired:
            return False, None, "Execution timeout"
        except Exception as e:
            return False, None, f"Unexpected error: {str(e)}"
    
    def test_single_operation(self, operation_name: str, mlir_file: str, llvm_file: str) -> Dict:
        """æµ‹è¯•å•ä¸ªæ“ä½œ"""
        print(f"  æµ‹è¯•å•ä¸ªæ“ä½œ: {operation_name}")
        
        result = {
            "operation": operation_name,
            "mlir_file": mlir_file,
            "llvm_file": llvm_file,
            "input_data": [],
            "output_data": None,
            "success": False,
            "error": None,
            "execution_time": 0,
            "input_shapes": [],
            "output_shape": None
        }
        
        try:
            # è§£æMLIRæ–‡ä»¶è·å–è¾“å…¥è¾“å‡ºå½¢çŠ¶
            input_shapes, output_shape = self.parse_mlir_shapes(mlir_file)
            result["input_shapes"] = input_shapes
            result["output_shape"] = output_shape
            
            # ç”Ÿæˆè¾“å…¥æ•°æ®
            input_data = []
            for shape in input_shapes:
                if len(shape) == 4:  # NHWCæ ¼å¼
                    config = self.input_configs["tensor_1x8x8x8"]
                elif len(shape) == 2:  # çŸ©é˜µæ ¼å¼
                    config = self.input_configs["tensor_16x16"]
                else:
                    config = self.input_configs["tensor_1x8x8x8"]
                
                data = self.generate_input_data(config)
                input_data.append(data)
                result["input_data"].append(data.tolist())
            
            # åˆ›å»ºCåŒ…è£…å™¨
            c_wrapper = self.create_c_wrapper(operation_name, input_shapes, output_shape, llvm_file)
            
            # è¿è¡Œæµ‹è¯•
            start_time = time.time()
            success, output_data, error = self.compile_and_run_llvm(
                llvm_file, c_wrapper, input_data, operation_name
            )
            end_time = time.time()
            
            result["success"] = success
            result["execution_time"] = end_time - start_time
            
            if success and output_data is not None:
                result["output_data"] = output_data.tolist()
                result["output_shape"] = list(output_data.shape)
            else:
                result["error"] = error
                
        except Exception as e:
            result["error"] = f"Test failed: {str(e)}"
        
        return result
    
    def parse_mlir_shapes(self, mlir_file: str) -> Tuple[List[List[int]], List[int]]:
        """è§£æMLIRæ–‡ä»¶è·å–è¾“å…¥è¾“å‡ºå½¢çŠ¶"""
        try:
            with open(mlir_file, 'r') as f:
                content = f.read()
            
            # ç®€å•çš„å½¢çŠ¶è§£æï¼ˆåŸºäºå·²çŸ¥æ ¼å¼ï¼‰
            # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„MLIRæ–‡ä»¶æ ¼å¼è¿›è¡Œè°ƒæ•´
            if "tensor<1x8x8x8xf32>" in content:
                input_shapes = [[1, 8, 8, 8], [1, 8, 8, 8]]
                output_shape = [1, 8, 8, 8]
            elif "tensor<16x16xf32>" in content:
                input_shapes = [[16, 16], [16, 16]]
                output_shape = [16, 16]
            else:
                # é»˜è®¤å½¢çŠ¶
                input_shapes = [[1, 8, 8, 8], [1, 8, 8, 8]]
                output_shape = [1, 8, 8, 8]
            
            return input_shapes, output_shape
            
        except Exception as e:
            print(f"Warning: Could not parse shapes from {mlir_file}: {e}")
            return [[1, 8, 8, 8], [1, 8, 8, 8]], [1, 8, 8, 8]
    
    def test_all_single_operations(self, sample_size: int = None):
        """æµ‹è¯•æ‰€æœ‰å•ä¸ªæ“ä½œ"""
        print("ğŸ”§ æµ‹è¯•æ‰€æœ‰å•ä¸ªæ“ä½œ...")
        
        # è·å–æ‰€æœ‰å•ä¸ªæ“ä½œ
        single_ops = []
        if os.path.exists("out/single"):
            single_ops = [d for d in os.listdir("out/single") 
                         if os.path.isdir(os.path.join("out/single", d))]
        
        if sample_size:
            import random
            single_ops = random.sample(single_ops, min(sample_size, len(single_ops)))
        
        for op in single_ops:
            mlir_file = f"out/single/{op}/{op}_N1_H8_W8_C8.mlir"
            llvm_file = f"out/single_llvm/{op}/{op}_N1_H8_W8_C8/{op}_N1_H8_W8_C8.ll"
            
            if os.path.exists(mlir_file) and os.path.exists(llvm_file):
                result = self.test_single_operation(op, mlir_file, llvm_file)
                self.dataset["single_operations"][op] = result
                
                if result["success"]:
                    self.dataset["metadata"]["successful_tests"] += 1
                else:
                    self.dataset["metadata"]["failed_tests"] += 1
                    print(f"    âŒ {op}: {result['error']}")
            else:
                print(f"    âš ï¸  {op}: æ–‡ä»¶ä¸å­˜åœ¨")
        
        self.dataset["metadata"]["single_operations"] = len(self.dataset["single_operations"])
    
    def test_all_pair_operations(self, sample_size: int = None):
        """æµ‹è¯•æ‰€æœ‰æˆå¯¹æ“ä½œ"""
        print("ğŸ”— æµ‹è¯•æ‰€æœ‰æˆå¯¹æ“ä½œ...")
        
        # è·å–æ‰€æœ‰æˆå¯¹æ“ä½œ
        pair_ops = []
        if os.path.exists("out/pairs_llvm"):
            pair_ops = [d for d in os.listdir("out/pairs_llvm") 
                       if os.path.isdir(os.path.join("out/pairs_llvm", d))]
        
        if sample_size:
            import random
            pair_ops = random.sample(pair_ops, min(sample_size, len(pair_ops)))
        
        for pair_name in pair_ops:
            print(f"  æµ‹è¯•æˆå¯¹æ“ä½œ: {pair_name}")
            
            # æŸ¥æ‰¾LLVMæ–‡ä»¶
            llvm_dir = f"out/pairs_llvm/{pair_name}"
            llvm_files = []
            if os.path.exists(llvm_dir):
                for root, dirs, files in os.walk(llvm_dir):
                    llvm_files.extend([os.path.join(root, f) for f in files if f.endswith('.ll')])
            
            if llvm_files:
                # æµ‹è¯•ç¬¬ä¸€ä¸ªLLVMæ–‡ä»¶
                llvm_file = llvm_files[0]
                result = self.test_single_operation(pair_name, "", llvm_file)
                result["pair_type"] = "sequential" if "_then_" in pair_name else "combined"
                result["llvm_files"] = llvm_files
                
                self.dataset["pair_operations"][pair_name] = result
                
                if result["success"]:
                    self.dataset["metadata"]["successful_tests"] += 1
                else:
                    self.dataset["metadata"]["failed_tests"] += 1
                    print(f"    âŒ {pair_name}: {result['error']}")
            else:
                print(f"    âš ï¸  {pair_name}: æœªæ‰¾åˆ°LLVMæ–‡ä»¶")
        
        self.dataset["metadata"]["pair_operations"] = len(self.dataset["pair_operations"])
    
    def generate_statistics(self):
        """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        print("ğŸ“Š ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯...")
        
        stats = {
            "total_operations": len(self.dataset["single_operations"]) + len(self.dataset["pair_operations"]),
            "successful_operations": 0,
            "failed_operations": 0,
            "average_execution_time": 0,
            "input_data_distribution": {},
            "output_data_distribution": {}
        }
        
        all_times = []
        
        # ç»Ÿè®¡å•ä¸ªæ“ä½œ
        for op, result in self.dataset["single_operations"].items():
            if result["success"]:
                stats["successful_operations"] += 1
                all_times.append(result["execution_time"])
            else:
                stats["failed_operations"] += 1
        
        # ç»Ÿè®¡æˆå¯¹æ“ä½œ
        for op, result in self.dataset["pair_operations"].items():
            if result["success"]:
                stats["successful_operations"] += 1
                all_times.append(result["execution_time"])
            else:
                stats["failed_operations"] += 1
        
        if all_times:
            stats["average_execution_time"] = sum(all_times) / len(all_times)
        
        self.dataset["statistics"] = stats
    
    def save_dataset(self):
        """ä¿å­˜æ•°æ®é›†"""
        print("ğŸ’¾ ä¿å­˜æ•°æ®é›†...")
        
        # ä¿å­˜å®Œæ•´æ•°æ®é›†
        dataset_file = self.output_dir / "io_dataset.json"
        with open(dataset_file, 'w', encoding='utf-8') as f:
            json.dump(self.dataset, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜ç®€åŒ–ç‰ˆæœ¬ï¼ˆåªåŒ…å«æˆåŠŸçš„æ•°æ®ï¼‰
        simplified_dataset = {
            "metadata": self.dataset["metadata"],
            "successful_single_operations": {
                k: v for k, v in self.dataset["single_operations"].items() 
                if v["success"]
            },
            "successful_pair_operations": {
                k: v for k, v in self.dataset["pair_operations"].items() 
                if v["success"]
            },
            "statistics": self.dataset["statistics"]
        }
        
        simplified_file = self.output_dir / "io_dataset_simplified.json"
        with open(simplified_file, 'w', encoding='utf-8') as f:
            json.dump(simplified_dataset, f, indent=2, ensure_ascii=False)
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        self.generate_markdown_report()
        
        print(f"âœ… æ•°æ®é›†å·²ä¿å­˜:")
        print(f"  ğŸ“ å®Œæ•´æ•°æ®é›†: {dataset_file}")
        print(f"  ğŸ“ ç®€åŒ–æ•°æ®é›†: {simplified_file}")
    
    def generate_markdown_report(self):
        """ç”ŸæˆMarkdownæŠ¥å‘Š"""
        report_file = self.output_dir / "io_dataset_report.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# è¾“å…¥-è¾“å‡º-IRæ•°æ®é›†æŠ¥å‘Š\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {self.dataset['metadata']['generated_at']}\n")
            f.write(f"**æ€»æ“ä½œæ•°**: {self.dataset['metadata']['total_operations']}\n")
            f.write(f"**å•ä¸ªæ“ä½œæ•°**: {self.dataset['metadata']['single_operations']}\n")
            f.write(f"**æˆå¯¹æ“ä½œæ•°**: {self.dataset['metadata']['pair_operations']}\n")
            f.write(f"**æˆåŠŸæµ‹è¯•æ•°**: {self.dataset['metadata']['successful_tests']}\n")
            f.write(f"**å¤±è´¥æµ‹è¯•æ•°**: {self.dataset['metadata']['failed_tests']}\n\n")
            
            # ç»Ÿè®¡ä¿¡æ¯
            stats = self.dataset["statistics"]
            f.write("## ğŸ“Š ç»Ÿè®¡ä¿¡æ¯\n\n")
            f.write(f"- **æ€»æ“ä½œæ•°**: {stats['total_operations']}\n")
            f.write(f"- **æˆåŠŸæ“ä½œæ•°**: {stats['successful_operations']}\n")
            f.write(f"- **å¤±è´¥æ“ä½œæ•°**: {stats['failed_operations']}\n")
            f.write(f"- **å¹³å‡æ‰§è¡Œæ—¶é—´**: {stats['average_execution_time']:.4f}ç§’\n\n")
            
            # æˆåŠŸæ“ä½œåˆ—è¡¨
            f.write("## âœ… æˆåŠŸæ“ä½œåˆ—è¡¨\n\n")
            f.write("### å•ä¸ªæ“ä½œ\n\n")
            for op, result in self.dataset["single_operations"].items():
                if result["success"]:
                    f.write(f"- **{op}**: æ‰§è¡Œæ—¶é—´ {result['execution_time']:.4f}ç§’\n")
            
            f.write("\n### æˆå¯¹æ“ä½œ\n\n")
            for op, result in self.dataset["pair_operations"].items():
                if result["success"]:
                    f.write(f"- **{op}**: æ‰§è¡Œæ—¶é—´ {result['execution_time']:.4f}ç§’\n")
            
            # å¤±è´¥æ“ä½œåˆ—è¡¨
            failed_ops = []
            for op, result in self.dataset["single_operations"].items():
                if not result["success"]:
                    failed_ops.append((op, result["error"]))
            for op, result in self.dataset["pair_operations"].items():
                if not result["success"]:
                    failed_ops.append((op, result["error"]))
            
            if failed_ops:
                f.write("\n## âŒ å¤±è´¥æ“ä½œåˆ—è¡¨\n\n")
                for op, error in failed_ops:
                    f.write(f"- **{op}**: {error}\n")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹ç”Ÿæˆè¾“å…¥-è¾“å‡º-IRæ•°æ®é›†...")
    
    generator = InputOutputGenerator()
    
    # æµ‹è¯•å•ä¸ªæ“ä½œï¼ˆé‡‡æ ·æµ‹è¯•ï¼‰
    generator.test_all_single_operations(sample_size=10)
    
    # æµ‹è¯•æˆå¯¹æ“ä½œï¼ˆé‡‡æ ·æµ‹è¯•ï¼‰
    generator.test_all_pair_operations(sample_size=5)
    
    # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
    generator.generate_statistics()
    
    # ä¿å­˜æ•°æ®é›†
    generator.save_dataset()
    
    print("âœ… è¾“å…¥-è¾“å‡º-IRæ•°æ®é›†ç”Ÿæˆå®Œæˆï¼")
    print(f"ğŸ“Š æ€»æ“ä½œæ•°: {generator.dataset['metadata']['total_operations']}")
    print(f"âœ… æˆåŠŸæµ‹è¯•: {generator.dataset['metadata']['successful_tests']}")
    print(f"âŒ å¤±è´¥æµ‹è¯•: {generator.dataset['metadata']['failed_tests']}")

if __name__ == "__main__":
    main()
