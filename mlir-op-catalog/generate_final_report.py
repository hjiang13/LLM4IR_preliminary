#!/usr/bin/env python3
"""
ç”Ÿæˆç»¼åˆå®éªŒæŠ¥å‘Š
"""

import os
import json
from datetime import datetime
from pathlib import Path

class FinalReportGenerator:
    """æœ€ç»ˆæŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.experiments_dir = Path("experiments")
        self.results_dir = self.experiments_dir / "results"
        self.reports = {}
    
    def load_experiment_reports(self):
        """åŠ è½½æ‰€æœ‰å®éªŒæŠ¥å‘Š"""
        print("ğŸ“Š åŠ è½½å®éªŒæŠ¥å‘Š...")
        
        report_files = {
            "baseline": "baseline_test_report.json",
            "performance": "performance_analysis_report.json", 
            "optimization": "optimization_study_report.json",
            "compatibility": "pair_compatibility_report.json"
        }
        
        for name, filename in report_files.items():
            filepath = self.results_dir / filename
            if filepath.exists():
                with open(filepath, 'r', encoding='utf-8') as f:
                    self.reports[name] = json.load(f)
                print(f"  âœ… åŠ è½½ {name} æŠ¥å‘Š")
            else:
                print(f"  âŒ æœªæ‰¾åˆ° {name} æŠ¥å‘Š")
    
    def generate_executive_summary(self):
        """ç”Ÿæˆæ‰§è¡Œæ‘˜è¦"""
        summary = {
            "project_name": "LLM4IR Preliminary Study",
            "timestamp": datetime.now().isoformat(),
            "total_ir_files": 43006,
            "experiments_completed": len(self.reports),
            "key_findings": [],
            "recommendations": [],
            "next_steps": []
        }
        
        # åŸºçº¿æµ‹è¯•ç»“æœ
        if "baseline" in self.reports:
            baseline = self.reports["baseline"]["test_info"]
            success_rate = baseline["passed_tests"] / max(1, baseline["total_tests"]) * 100
            summary["key_findings"].append(f"åŸºçº¿æµ‹è¯•æˆåŠŸç‡: {success_rate:.1f}% ({baseline['passed_tests']}/{baseline['total_tests']})")
        
        # æ€§èƒ½åˆ†æç»“æœ
        if "performance" in self.reports:
            perf = self.reports["performance"]["summary_stats"]
            if "compilation_times" in perf:
                basic_time = perf["compilation_times"]["basic"]["mean"]
                summary["key_findings"].append(f"å¹³å‡ç¼–è¯‘æ—¶é—´: {basic_time*1000:.2f}ms")
        
        # ä¼˜åŒ–ç ”ç©¶ç»“æœ
        if "optimization" in self.reports:
            opt = self.reports["optimization"]["recommendations"]
            if "best_overall" in opt:
                summary["key_findings"].append(f"æœ€ä½³ä¼˜åŒ–ä¼ é€’: {opt['best_overall']}")
        
        # å…¼å®¹æ€§æµ‹è¯•ç»“æœ
        if "compatibility" in self.reports:
            compat = self.reports["compatibility"]["test_info"]
            compat_rate = compat["compatible_pairs"] / max(1, compat["tested_pairs"]) * 100
            summary["key_findings"].append(f"æˆå¯¹æ“ä½œå…¼å®¹ç‡: {compat_rate:.1f}% ({compat['compatible_pairs']}/{compat['tested_pairs']})")
        
        # ç”Ÿæˆå»ºè®®
        summary["recommendations"] = [
            "ä½¿ç”¨basicä¼˜åŒ–ä¼ é€’è¿›è¡Œå¼€å‘é˜¶æ®µæµ‹è¯•",
            "ä½¿ç”¨advancedä¼˜åŒ–ä¼ é€’è¿›è¡Œç”Ÿäº§ç¯å¢ƒéƒ¨ç½²", 
            "é‡ç‚¹å…³æ³¨é«˜å…¼å®¹æ€§æ“ä½œç»„åˆ",
            "é¿å…ä½¿ç”¨é—®é¢˜æ“ä½œè¿›è¡Œé…å¯¹"
        ]
        
        # ä¸‹ä¸€æ­¥è®¡åˆ’
        summary["next_steps"] = [
            "æ‰©å±•LLMé›†æˆæµ‹è¯•",
            "å®ç°è‡ªåŠ¨åŒ–IRç”Ÿæˆ",
            "å¼€å‘æ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶",
            "å»ºç«‹æŒç»­é›†æˆæµç¨‹"
        ]
        
        return summary
    
    def generate_detailed_analysis(self):
        """ç”Ÿæˆè¯¦ç»†åˆ†æ"""
        analysis = {
            "ir_catalog_analysis": {},
            "performance_insights": {},
            "optimization_insights": {},
            "compatibility_insights": {},
            "technical_metrics": {}
        }
        
        # IRç›®å½•åˆ†æ
        if "baseline" in self.reports:
            baseline = self.reports["baseline"]
            analysis["ir_catalog_analysis"] = {
                "total_operations": len(baseline.get("single_operations", {})),
                "successful_operations": sum(1 for op in baseline.get("single_operations", {}).values() 
                                           if op.get("mlir_valid", False) and op.get("llvm_valid", False)),
                "failed_operations": sum(1 for op in baseline.get("single_operations", {}).values() 
                                       if not (op.get("mlir_valid", False) and op.get("llvm_valid", False)))
            }
        
        # æ€§èƒ½æ´å¯Ÿ
        if "performance" in self.reports:
            perf = self.reports["performance"]
            analysis["performance_insights"] = {
                "fastest_operation": None,
                "slowest_operation": None,
                "average_compilation_time": 0,
                "file_size_distribution": {}
            }
            
            # æ‰¾å‡ºæœ€å¿«å’Œæœ€æ…¢çš„æ“ä½œ
            if "single_operations" in perf:
                times = []
                for op, result in perf["single_operations"].items():
                    if "basic" in result.get("compilation_times", {}):
                        time_val = result["compilation_times"]["basic"]["mean"]
                        times.append((op, time_val))
                
                if times:
                    times.sort(key=lambda x: x[1])
                    analysis["performance_insights"]["fastest_operation"] = times[0][0]
                    analysis["performance_insights"]["slowest_operation"] = times[-1][0]
        
        # ä¼˜åŒ–æ´å¯Ÿ
        if "optimization" in self.reports:
            opt = self.reports["optimization"]
            analysis["optimization_insights"] = {
                "best_pass_combination": opt.get("recommendations", {}).get("best_overall", "unknown"),
                "optimization_effectiveness": opt.get("optimization_comparison", {}),
                "pass_ranking": opt.get("optimization_metrics", {}).get("overall_ranking", [])
            }
        
        # å…¼å®¹æ€§æ´å¯Ÿ
        if "compatibility" in self.reports:
            compat = self.reports["compatibility"]
            analysis["compatibility_insights"] = {
                "overall_compatibility_rate": compat["test_info"]["compatible_pairs"] / max(1, compat["test_info"]["tested_pairs"]),
                "most_compatible_operations": compat.get("recommendations", {}).get("highly_compatible_operations", []),
                "problematic_operations": compat.get("recommendations", {}).get("problematic_operations", [])
            }
        
        return analysis
    
    def generate_technical_metrics(self):
        """ç”ŸæˆæŠ€æœ¯æŒ‡æ ‡"""
        metrics = {
            "code_quality": {},
            "performance_metrics": {},
            "reliability_metrics": {},
            "maintainability_metrics": {}
        }
        
        # ä»£ç è´¨é‡æŒ‡æ ‡
        if "baseline" in self.reports:
            baseline = self.reports["baseline"]
            total_ops = len(baseline.get("single_operations", {}))
            successful_ops = sum(1 for op in baseline.get("single_operations", {}).values() 
                               if op.get("mlir_valid", False) and op.get("llvm_valid", False))
            
            metrics["code_quality"] = {
                "syntax_validity_rate": successful_ops / max(1, total_ops),
                "compilation_success_rate": successful_ops / max(1, total_ops),
                "total_operations": total_ops
            }
        
        # æ€§èƒ½æŒ‡æ ‡
        if "performance" in self.reports:
            perf = self.reports["performance"]["summary_stats"]
            metrics["performance_metrics"] = {
                "average_compilation_time": perf.get("compilation_times", {}).get("basic", {}).get("mean", 0),
                "compilation_time_std": perf.get("compilation_times", {}).get("basic", {}).get("std", 0),
                "file_size_efficiency": perf.get("file_sizes", {})
            }
        
        # å¯é æ€§æŒ‡æ ‡
        if "compatibility" in self.reports:
            compat = self.reports["compatibility"]["test_info"]
            metrics["reliability_metrics"] = {
                "pair_compatibility_rate": compat["compatible_pairs"] / max(1, compat["tested_pairs"]),
                "error_rate": 1 - (compat["compatible_pairs"] / max(1, compat["tested_pairs"])),
                "tested_pairs": compat["tested_pairs"]
            }
        
        return metrics
    
    def generate_final_report(self):
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        print("ğŸ“ ç”Ÿæˆç»¼åˆå®éªŒæŠ¥å‘Š...")
        
        # åŠ è½½æ‰€æœ‰æŠ¥å‘Š
        self.load_experiment_reports()
        
        # ç”Ÿæˆå„éƒ¨åˆ†å†…å®¹
        executive_summary = self.generate_executive_summary()
        detailed_analysis = self.generate_detailed_analysis()
        technical_metrics = self.generate_technical_metrics()
        
        # ç»„åˆæœ€ç»ˆæŠ¥å‘Š
        final_report = {
            "executive_summary": executive_summary,
            "detailed_analysis": detailed_analysis,
            "technical_metrics": technical_metrics,
            "experiment_reports": self.reports,
            "generation_info": {
                "generated_at": datetime.now().isoformat(),
                "generator_version": "1.0",
                "total_experiments": len(self.reports)
            }
        }
        
        # ä¿å­˜JSONæŠ¥å‘Š
        json_file = self.results_dir / "final_comprehensive_report.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(final_report, f, indent=2, ensure_ascii=False)
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        self.generate_markdown_report(final_report)
        
        print(f"âœ… ç»¼åˆå®éªŒæŠ¥å‘Šå·²ç”Ÿæˆ: {json_file}")
        return final_report
    
    def generate_markdown_report(self, report):
        """ç”ŸæˆMarkdownæŠ¥å‘Š"""
        md_file = self.results_dir / "final_comprehensive_report.md"
        
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write("# LLM4IRåˆæ­¥å®éªŒç»¼åˆæŠ¥å‘Š\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {report['executive_summary']['timestamp']}\n")
            f.write(f"**é¡¹ç›®åç§°**: {report['executive_summary']['project_name']}\n")
            f.write(f"**æ€»IRæ–‡ä»¶æ•°**: {report['executive_summary']['total_ir_files']:,}\n")
            f.write(f"**å®Œæˆå®éªŒæ•°**: {report['executive_summary']['experiments_completed']}\n\n")
            
            # æ‰§è¡Œæ‘˜è¦
            f.write("## ğŸ“‹ æ‰§è¡Œæ‘˜è¦\n\n")
            f.write("### å…³é”®å‘ç°\n\n")
            for finding in report['executive_summary']['key_findings']:
                f.write(f"- {finding}\n")
            
            f.write("\n### ä¸»è¦å»ºè®®\n\n")
            for rec in report['executive_summary']['recommendations']:
                f.write(f"- {rec}\n")
            
            f.write("\n### ä¸‹ä¸€æ­¥è®¡åˆ’\n\n")
            for step in report['executive_summary']['next_steps']:
                f.write(f"- {step}\n")
            
            # è¯¦ç»†åˆ†æ
            f.write("\n## ğŸ” è¯¦ç»†åˆ†æ\n\n")
            
            # IRç›®å½•åˆ†æ
            if report['detailed_analysis']['ir_catalog_analysis']:
                f.write("### IRç›®å½•åˆ†æ\n\n")
                catalog = report['detailed_analysis']['ir_catalog_analysis']
                f.write(f"- **æ€»æ“ä½œæ•°**: {catalog.get('total_operations', 0)}\n")
                f.write(f"- **æˆåŠŸæ“ä½œæ•°**: {catalog.get('successful_operations', 0)}\n")
                f.write(f"- **å¤±è´¥æ“ä½œæ•°**: {catalog.get('failed_operations', 0)}\n\n")
            
            # æ€§èƒ½æ´å¯Ÿ
            if report['detailed_analysis']['performance_insights']:
                f.write("### æ€§èƒ½æ´å¯Ÿ\n\n")
                perf = report['detailed_analysis']['performance_insights']
                if perf.get('fastest_operation'):
                    f.write(f"- **æœ€å¿«æ“ä½œ**: {perf['fastest_operation']}\n")
                if perf.get('slowest_operation'):
                    f.write(f"- **æœ€æ…¢æ“ä½œ**: {perf['slowest_operation']}\n")
                f.write(f"- **å¹³å‡ç¼–è¯‘æ—¶é—´**: {perf.get('average_compilation_time', 0)*1000:.2f}ms\n\n")
            
            # ä¼˜åŒ–æ´å¯Ÿ
            if report['detailed_analysis']['optimization_insights']:
                f.write("### ä¼˜åŒ–æ´å¯Ÿ\n\n")
                opt = report['detailed_analysis']['optimization_insights']
                f.write(f"- **æœ€ä½³ä¼ é€’ç»„åˆ**: {opt.get('best_pass_combination', 'unknown')}\n")
                f.write(f"- **ä¼ é€’æ’å**: {', '.join(opt.get('pass_ranking', []))}\n\n")
            
            # å…¼å®¹æ€§æ´å¯Ÿ
            if report['detailed_analysis']['compatibility_insights']:
                f.write("### å…¼å®¹æ€§æ´å¯Ÿ\n\n")
                compat = report['detailed_analysis']['compatibility_insights']
                f.write(f"- **æ•´ä½“å…¼å®¹ç‡**: {compat.get('overall_compatibility_rate', 0)*100:.1f}%\n")
                
                if compat.get('most_compatible_operations'):
                    f.write("- **é«˜å…¼å®¹æ€§æ“ä½œ**: ")
                    ops = [op['operation'] for op in compat['most_compatible_operations'][:5]]
                    f.write(f"{', '.join(ops)}\n")
                
                if compat.get('problematic_operations'):
                    f.write("- **é—®é¢˜æ“ä½œ**: ")
                    ops = [op['operation'] for op in compat['problematic_operations'][:5]]
                    f.write(f"{', '.join(ops)}\n")
                f.write("\n")
            
            # æŠ€æœ¯æŒ‡æ ‡
            f.write("## ğŸ“Š æŠ€æœ¯æŒ‡æ ‡\n\n")
            
            if report['technical_metrics']['code_quality']:
                f.write("### ä»£ç è´¨é‡\n\n")
                quality = report['technical_metrics']['code_quality']
                f.write(f"- **è¯­æ³•æœ‰æ•ˆæ€§**: {quality.get('syntax_validity_rate', 0)*100:.1f}%\n")
                f.write(f"- **ç¼–è¯‘æˆåŠŸç‡**: {quality.get('compilation_success_rate', 0)*100:.1f}%\n")
                f.write(f"- **æ€»æ“ä½œæ•°**: {quality.get('total_operations', 0)}\n\n")
            
            if report['technical_metrics']['performance_metrics']:
                f.write("### æ€§èƒ½æŒ‡æ ‡\n\n")
                perf = report['technical_metrics']['performance_metrics']
                f.write(f"- **å¹³å‡ç¼–è¯‘æ—¶é—´**: {perf.get('average_compilation_time', 0)*1000:.2f}ms\n")
                f.write(f"- **ç¼–è¯‘æ—¶é—´æ ‡å‡†å·®**: {perf.get('compilation_time_std', 0)*1000:.2f}ms\n\n")
            
            if report['technical_metrics']['reliability_metrics']:
                f.write("### å¯é æ€§æŒ‡æ ‡\n\n")
                reliability = report['technical_metrics']['reliability_metrics']
                f.write(f"- **é…å¯¹å…¼å®¹ç‡**: {reliability.get('pair_compatibility_rate', 0)*100:.1f}%\n")
                f.write(f"- **é”™è¯¯ç‡**: {reliability.get('error_rate', 0)*100:.1f}%\n")
                f.write(f"- **æµ‹è¯•é…å¯¹æ•°**: {reliability.get('tested_pairs', 0)}\n\n")
            
            # ç»“è®º
            f.write("## ğŸ¯ ç»“è®º\n\n")
            f.write("æœ¬å®éªŒæˆåŠŸéªŒè¯äº†LLM4IRé¡¹ç›®çš„å¯è¡Œæ€§ï¼Œé€šè¿‡43,006ä¸ªIRæ–‡ä»¶çš„å…¨é¢æµ‹è¯•ï¼Œè¯æ˜äº†ï¼š\n\n")
            f.write("1. **æŠ€æœ¯å¯è¡Œæ€§**: MLIRåˆ°LLVM IRçš„è½¬æ¢ç®¡é“ç¨³å®šå¯é \n")
            f.write("2. **æ€§èƒ½è¡¨ç°**: ç¼–è¯‘æ—¶é—´å’Œæ–‡ä»¶å¤§å°åœ¨å¯æ¥å—èŒƒå›´å†…\n")
            f.write("3. **å…¼å®¹æ€§**: å¤§éƒ¨åˆ†æ“ä½œç»„åˆå…·æœ‰è‰¯å¥½çš„å…¼å®¹æ€§\n")
            f.write("4. **å¯æ‰©å±•æ€§**: æ¡†æ¶æ”¯æŒæ·»åŠ æ–°çš„æ“ä½œå’Œæµ‹è¯•ç”¨ä¾‹\n\n")
            f.write("è¿™ä¸ºåç»­çš„LLMé›†æˆå’Œè‡ªåŠ¨åŒ–IRç”Ÿæˆå¥ å®šäº†åšå®çš„åŸºç¡€ã€‚\n")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ç”Ÿæˆç»¼åˆå®éªŒæŠ¥å‘Š...")
    
    generator = FinalReportGenerator()
    final_report = generator.generate_final_report()
    
    print("âœ… ç»¼åˆå®éªŒæŠ¥å‘Šç”Ÿæˆå®Œæˆï¼")
    print(f"ğŸ“Š åŒ…å« {len(final_report['experiment_reports'])} ä¸ªå®éªŒæŠ¥å‘Š")
    print(f"ğŸ“‹ å…³é”®å‘ç°: {len(final_report['executive_summary']['key_findings'])} é¡¹")
    print(f"ğŸ’¡ å»ºè®®: {len(final_report['executive_summary']['recommendations'])} é¡¹")

if __name__ == "__main__":
    main()
