#!/usr/bin/env python3
"""
ÊÄßËÉΩÂàÜÊûê - ÊµãÈáè‰∏çÂêåÊìç‰ΩúÁöÑÊâßË°åÊó∂Èó¥
"""

import os
import json
import subprocess
import time
import statistics
from pathlib import Path
from datetime import datetime

class PerformanceAnalysis:
    """ÊÄßËÉΩÂàÜÊûêÁ±ª"""
    
    def __init__(self):
        self.results = {
            "analysis_info": {
                "timestamp": datetime.now().isoformat(),
                "analysis_type": "performance_analysis",
                "total_operations": 0,
                "analyzed_operations": 0
            },
            "single_operations": {},
            "pair_operations": {},
            "optimization_comparison": {},
            "summary_stats": {}
        }
    
    def measure_mlir_compilation_time(self, mlir_file, passes, iterations=10):
        """ÊµãÈáèMLIRÁºñËØëÊó∂Èó¥"""
        times = []
        
        for _ in range(iterations):
            start_time = time.time()
            try:
                cmd = ["mlir-opt", mlir_file] + passes
                result = subprocess.run(
                    cmd,
                    capture_output=True,
                    text=True,
                    timeout=60
                )
                end_time = time.time()
                
                if result.returncode == 0:
                    times.append(end_time - start_time)
            except (subprocess.TimeoutExpired, Exception):
                continue
        
        if times:
            return {
                "mean": statistics.mean(times),
                "median": statistics.median(times),
                "min": min(times),
                "max": max(times),
                "std": statistics.stdev(times) if len(times) > 1 else 0,
                "samples": len(times)
            }
        return None
    
    def measure_llvm_compilation_time(self, llvm_file, iterations=10):
        """ÊµãÈáèLLVMÁºñËØëÊó∂Èó¥"""
        times = []
        
        for _ in range(iterations):
            start_time = time.time()
            try:
                result = subprocess.run(
                    ["llvm-as", llvm_file, "-o", "/tmp/test.bc"],
                    capture_output=True,
                    text=True,
                    timeout=30
                )
                end_time = time.time()
                
                if result.returncode == 0:
                    times.append(end_time - start_time)
                    os.remove("/tmp/test.bc")
            except (subprocess.TimeoutExpired, Exception):
                continue
        
        if times:
            return {
                "mean": statistics.mean(times),
                "median": statistics.median(times),
                "min": min(times),
                "max": max(times),
                "std": statistics.stdev(times) if len(times) > 1 else 0,
                "samples": len(times)
            }
        return None
    
    def get_file_size(self, file_path):
        """Ëé∑ÂèñÊñá‰ª∂Â§ßÂ∞è"""
        try:
            return os.path.getsize(file_path)
        except OSError:
            return 0
    
    def analyze_single_operations(self, sample_size=20):
        """ÂàÜÊûêÂçï‰∏™Êìç‰ΩúÊÄßËÉΩ"""
        print("üîß ÂàÜÊûêÂçï‰∏™Êìç‰ΩúÊÄßËÉΩ...")
        
        # Ëé∑ÂèñÊâÄÊúâÂçï‰∏™Êìç‰Ωú
        single_ops = []
        if os.path.exists("out/single"):
            single_ops = [d for d in os.listdir("out/single") 
                         if os.path.isdir(os.path.join("out/single", d))]
        
        # ÈöèÊú∫ÈááÊ†∑
        import random
        sample_ops = random.sample(single_ops, min(sample_size, len(single_ops)))
        
        for op in sample_ops:
            print(f"  ÂàÜÊûêÊìç‰Ωú: {op}")
            
            mlir_file = f"out/single/{op}/{op}_N1_H8_W8_C8.mlir"
            llvm_file = f"out/single_llvm/{op}/{op}_N1_H8_W8_C8/{op}_N1_H8_W8_C8.ll"
            
            op_result = {
                "operation": op,
                "mlir_file": mlir_file,
                "llvm_file": llvm_file,
                "mlir_size": 0,
                "llvm_size": 0,
                "compilation_times": {},
                "errors": []
            }
            
            # Êñá‰ª∂Â§ßÂ∞è
            if os.path.exists(mlir_file):
                op_result["mlir_size"] = self.get_file_size(mlir_file)
            if os.path.exists(llvm_file):
                op_result["llvm_size"] = self.get_file_size(llvm_file)
            
            # ÁºñËØëÊó∂Èó¥ÊµãËØï
            if os.path.exists(mlir_file):
                # Âü∫Á°Ä‰ºòÂåñ‰º†ÈÄí
                basic_passes = ["--convert-linalg-to-loops", "--lower-affine", "--convert-scf-to-cf"]
                basic_time = self.measure_mlir_compilation_time(mlir_file, basic_passes, 5)
                if basic_time:
                    op_result["compilation_times"]["basic"] = basic_time
                
                # È´òÁ∫ß‰ºòÂåñ‰º†ÈÄí
                advanced_passes = basic_passes + ["--convert-math-to-llvm", "--convert-arith-to-llvm", "--convert-func-to-llvm"]
                advanced_time = self.measure_mlir_compilation_time(mlir_file, advanced_passes, 5)
                if advanced_time:
                    op_result["compilation_times"]["advanced"] = advanced_time
            
            # LLVMÁºñËØëÊó∂Èó¥
            if os.path.exists(llvm_file):
                llvm_time = self.measure_llvm_compilation_time(llvm_file, 5)
                if llvm_time:
                    op_result["compilation_times"]["llvm"] = llvm_time
            
            self.results["single_operations"][op] = op_result
            self.results["analysis_info"]["analyzed_operations"] += 1
    
    def analyze_optimization_effectiveness(self):
        """ÂàÜÊûê‰ºòÂåñÊïàÊûú"""
        print("‚ö° ÂàÜÊûê‰ºòÂåñÊïàÊûú...")
        
        optimization_stats = {
            "basic_vs_advanced": {},
            "file_size_impact": {},
            "compilation_time_impact": {}
        }
        
        # ÊØîËæÉÂü∫Á°ÄvsÈ´òÁ∫ß‰ºòÂåñ
        basic_times = []
        advanced_times = []
        
        for op, result in self.results["single_operations"].items():
            if "basic" in result["compilation_times"] and "advanced" in result["compilation_times"]:
                basic_times.append(result["compilation_times"]["basic"]["mean"])
                advanced_times.append(result["compilation_times"]["advanced"]["mean"])
        
        if basic_times and advanced_times:
            optimization_stats["basic_vs_advanced"] = {
                "basic_mean": statistics.mean(basic_times),
                "advanced_mean": statistics.mean(advanced_times),
                "improvement": (statistics.mean(basic_times) - statistics.mean(advanced_times)) / statistics.mean(basic_times) * 100
            }
        
        self.results["optimization_comparison"] = optimization_stats
    
    def generate_summary_statistics(self):
        """ÁîüÊàêÊ±áÊÄªÁªüËÆ°"""
        print("üìä ÁîüÊàêÊ±áÊÄªÁªüËÆ°...")
        
        # ÁºñËØëÊó∂Èó¥ÁªüËÆ°
        all_basic_times = []
        all_advanced_times = []
        all_llvm_times = []
        
        # Êñá‰ª∂Â§ßÂ∞èÁªüËÆ°
        mlir_sizes = []
        llvm_sizes = []
        
        for op, result in self.results["single_operations"].items():
            if "basic" in result["compilation_times"]:
                all_basic_times.append(result["compilation_times"]["basic"]["mean"])
            if "advanced" in result["compilation_times"]:
                all_advanced_times.append(result["compilation_times"]["advanced"]["mean"])
            if "llvm" in result["compilation_times"]:
                all_llvm_times.append(result["compilation_times"]["llvm"]["mean"])
            
            if result["mlir_size"] > 0:
                mlir_sizes.append(result["mlir_size"])
            if result["llvm_size"] > 0:
                llvm_sizes.append(result["llvm_size"])
        
        summary = {
            "compilation_times": {
                "basic": {
                    "mean": statistics.mean(all_basic_times) if all_basic_times else 0,
                    "median": statistics.median(all_basic_times) if all_basic_times else 0,
                    "min": min(all_basic_times) if all_basic_times else 0,
                    "max": max(all_basic_times) if all_basic_times else 0
                },
                "advanced": {
                    "mean": statistics.mean(all_advanced_times) if all_advanced_times else 0,
                    "median": statistics.median(all_advanced_times) if all_advanced_times else 0,
                    "min": min(all_advanced_times) if all_advanced_times else 0,
                    "max": max(all_advanced_times) if all_advanced_times else 0
                },
                "llvm": {
                    "mean": statistics.mean(all_llvm_times) if all_llvm_times else 0,
                    "median": statistics.median(all_llvm_times) if all_llvm_times else 0,
                    "min": min(all_llvm_times) if all_llvm_times else 0,
                    "max": max(all_llvm_times) if all_llvm_times else 0
                }
            },
            "file_sizes": {
                "mlir": {
                    "mean": statistics.mean(mlir_sizes) if mlir_sizes else 0,
                    "median": statistics.median(mlir_sizes) if mlir_sizes else 0,
                    "min": min(mlir_sizes) if mlir_sizes else 0,
                    "max": max(mlir_sizes) if mlir_sizes else 0
                },
                "llvm": {
                    "mean": statistics.mean(llvm_sizes) if llvm_sizes else 0,
                    "median": statistics.median(llvm_sizes) if llvm_sizes else 0,
                    "min": min(llvm_sizes) if llvm_sizes else 0,
                    "max": max(llvm_sizes) if llvm_sizes else 0
                }
            }
        }
        
        self.results["summary_stats"] = summary
    
    def generate_report(self):
        """ÁîüÊàêÊÄßËÉΩÂàÜÊûêÊä•Âëä"""
        report_file = "experiments/results/performance_analysis_report.json"
        
        # Á°Æ‰øùÁõÆÂΩïÂ≠òÂú®
        os.makedirs(os.path.dirname(report_file), exist_ok=True)
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        # ÁîüÊàêMarkdownÊä•Âëä
        self.generate_markdown_report()
        
        print(f"‚úÖ ÊÄßËÉΩÂàÜÊûêÊä•ÂëäÂ∑≤ÁîüÊàê: {report_file}")
    
    def generate_markdown_report(self):
        """ÁîüÊàêMarkdownÊä•Âëä"""
        report_file = "experiments/results/performance_analysis_report.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# ÊÄßËÉΩÂàÜÊûêÊä•Âëä\n\n")
            f.write(f"**ÂàÜÊûêÊó∂Èó¥**: {self.results['analysis_info']['timestamp']}\n")
            f.write(f"**ÂàÜÊûêÁ±ªÂûã**: {self.results['analysis_info']['analysis_type']}\n")
            f.write(f"**ÂàÜÊûêÊìç‰ΩúÊï∞**: {self.results['analysis_info']['analyzed_operations']}\n\n")
            
            # Ê±áÊÄªÁªüËÆ°
            f.write("## üìä Ê±áÊÄªÁªüËÆ°\n\n")
            summary = self.results["summary_stats"]
            
            f.write("### ÁºñËØëÊó∂Èó¥ÁªüËÆ° (Áßí)\n\n")
            f.write("| Á±ªÂûã | Âπ≥Âùá | ‰∏≠‰ΩçÊï∞ | ÊúÄÂ∞è | ÊúÄÂ§ß |\n")
            f.write("|------|------|--------|------|------|\n")
            
            for comp_type, stats in summary["compilation_times"].items():
                f.write(f"| {comp_type} | {stats['mean']:.4f} | {stats['median']:.4f} | {stats['min']:.4f} | {stats['max']:.4f} |\n")
            
            f.write("\n### Êñá‰ª∂Â§ßÂ∞èÁªüËÆ° (Â≠óËäÇ)\n\n")
            f.write("| Á±ªÂûã | Âπ≥Âùá | ‰∏≠‰ΩçÊï∞ | ÊúÄÂ∞è | ÊúÄÂ§ß |\n")
            f.write("|------|------|--------|------|------|\n")
            
            for file_type, stats in summary["file_sizes"].items():
                f.write(f"| {file_type} | {stats['mean']:.0f} | {stats['median']:.0f} | {stats['min']:.0f} | {stats['max']:.0f} |\n")
            
            # ‰ºòÂåñÊïàÊûú
            if "basic_vs_advanced" in self.results["optimization_comparison"]:
                f.write("\n## ‚ö° ‰ºòÂåñÊïàÊûúÂàÜÊûê\n\n")
                opt_stats = self.results["optimization_comparison"]["basic_vs_advanced"]
                f.write(f"- **Âü∫Á°Ä‰ºòÂåñÂπ≥ÂùáÊó∂Èó¥**: {opt_stats['basic_mean']:.4f}Áßí\n")
                f.write(f"- **È´òÁ∫ß‰ºòÂåñÂπ≥ÂùáÊó∂Èó¥**: {opt_stats['advanced_mean']:.4f}Áßí\n")
                f.write(f"- **ÊÄßËÉΩÊèêÂçá**: {opt_stats['improvement']:.1f}%\n\n")
            
            # ËØ¶ÁªÜÊìç‰ΩúÁªìÊûú
            f.write("## üîß ËØ¶ÁªÜÊìç‰ΩúÊÄßËÉΩ\n\n")
            f.write("| Êìç‰Ωú | Âü∫Á°ÄÁºñËØë(ms) | È´òÁ∫ßÁºñËØë(ms) | LLVMÁºñËØë(ms) | MLIRÂ§ßÂ∞è | LLVMÂ§ßÂ∞è |\n")
            f.write("|------|-------------|-------------|-------------|----------|----------|\n")
            
            for op, result in self.results["single_operations"].items():
                basic_time = result["compilation_times"].get("basic", {}).get("mean", 0) * 1000
                advanced_time = result["compilation_times"].get("advanced", {}).get("mean", 0) * 1000
                llvm_time = result["compilation_times"].get("llvm", {}).get("mean", 0) * 1000
                mlir_size = result["mlir_size"]
                llvm_size = result["llvm_size"]
                
                f.write(f"| {op} | {basic_time:.2f} | {advanced_time:.2f} | {llvm_time:.2f} | {mlir_size} | {llvm_size} |\n")

def main():
    """‰∏ªÂáΩÊï∞"""
    print("üöÄ ÂºÄÂßãÊÄßËÉΩÂàÜÊûê...")
    
    analysis = PerformanceAnalysis()
    
    # ÂàÜÊûêÂçï‰∏™Êìç‰Ωú
    analysis.analyze_single_operations(sample_size=15)
    
    # ÂàÜÊûê‰ºòÂåñÊïàÊûú
    analysis.analyze_optimization_effectiveness()
    
    # ÁîüÊàêÊ±áÊÄªÁªüËÆ°
    analysis.generate_summary_statistics()
    
    # ÁîüÊàêÊä•Âëä
    analysis.generate_report()
    
    print("‚úÖ ÊÄßËÉΩÂàÜÊûêÂÆåÊàêÔºÅ")
    print(f"üìä ÂàÜÊûêÊìç‰ΩúÊï∞: {analysis.results['analysis_info']['analyzed_operations']}")

if __name__ == "__main__":
    main()
