#!/usr/bin/env python3
"""
ç®€åŒ–çš„è¾“å…¥-è¾“å‡ºæµ‹è¯•
é€šè¿‡åˆ†æMLIRæ–‡ä»¶ç”Ÿæˆè¾“å…¥æ•°æ®ï¼Œå¹¶æ¨¡æ‹Ÿè¾“å‡ºç»“æœ
"""

import os
import json
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Any

class SimpleIOTest:
    """ç®€åŒ–çš„è¾“å…¥è¾“å‡ºæµ‹è¯•"""
    
    def __init__(self):
        self.base_dir = Path(".")
        self.output_dir = self.base_dir / "io_dataset"
        self.output_dir.mkdir(exist_ok=True)
        
        self.dataset = {
            "metadata": {
                "generated_at": datetime.now().isoformat(),
                "total_operations": 0,
                "single_operations": 0,
                "pair_operations": 0,
                "test_type": "simulated_io"
            },
            "single_operations": {},
            "pair_operations": {},
            "statistics": {}
        }
    
    def generate_test_data(self, shape: List[int], dtype: str = "float32") -> np.ndarray:
        """ç”Ÿæˆæµ‹è¯•æ•°æ®"""
        if dtype == "float32":
            # ç”Ÿæˆæœ‰æ„ä¹‰çš„æµ‹è¯•æ•°æ®
            if len(shape) == 4:  # NHWCæ ¼å¼
                data = np.random.uniform(0.0, 1.0, shape).astype(np.float32)
            elif len(shape) == 2:  # çŸ©é˜µæ ¼å¼
                data = np.random.uniform(-1.0, 1.0, shape).astype(np.float32)
            else:
                data = np.random.uniform(0.0, 1.0, shape).astype(np.float32)
        else:
            data = np.random.randint(0, 2, shape).astype(np.int32)
        
        return data
    
    def simulate_operation_output(self, operation: str, input_data: List[np.ndarray]) -> np.ndarray:
        """æ¨¡æ‹Ÿæ“ä½œè¾“å‡º"""
        if len(input_data) == 0:
            return np.array([])
        
        input1 = input_data[0]
        
        if operation == "add":
            if len(input_data) >= 2:
                return input1 + input_data[1]
            else:
                return input1
        elif operation == "mul":
            if len(input_data) >= 2:
                return input1 * input_data[1]
            else:
                return input1
        elif operation == "sub":
            if len(input_data) >= 2:
                return input1 - input_data[1]
            else:
                return input1
        elif operation == "div":
            if len(input_data) >= 2:
                return input1 / (input_data[1] + 1e-8)  # é¿å…é™¤é›¶
            else:
                return input1
        elif operation == "relu":
            return np.maximum(0, input1)
        elif operation == "sigmoid":
            return 1 / (1 + np.exp(-np.clip(input1, -500, 500)))
        elif operation == "tanh":
            return np.tanh(input1)
        elif operation == "sin":
            return np.sin(input1)
        elif operation == "cos":
            return np.cos(input1)
        elif operation == "exp":
            return np.exp(np.clip(input1, -500, 500))
        elif operation == "log":
            return np.log(np.maximum(input1, 1e-8))
        elif operation == "sqrt":
            return np.sqrt(np.maximum(input1, 0))
        elif operation == "abs":
            return np.abs(input1)
        elif operation == "matmul":
            if len(input_data) >= 2:
                return np.matmul(input1, input_data[1])
            else:
                return input1
        elif operation == "conv2d_nhwc_hwcf":
            # ç®€åŒ–çš„å·ç§¯æ¨¡æ‹Ÿ
            return input1  # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„å·ç§¯æ¨¡æ‹Ÿ
        elif operation == "maxpool2d":
            # ç®€åŒ–çš„æœ€å¤§æ± åŒ–æ¨¡æ‹Ÿ
            return input1
        elif operation == "avgpool2d":
            # ç®€åŒ–çš„å¹³å‡æ± åŒ–æ¨¡æ‹Ÿ
            return input1
        else:
            # é»˜è®¤è¿”å›è¾“å…¥
            return input1
    
    def parse_mlir_file(self, mlir_file: str) -> Tuple[List[List[int]], List[int], str]:
        """è§£æMLIRæ–‡ä»¶è·å–å½¢çŠ¶å’Œæ“ä½œç±»å‹"""
        try:
            with open(mlir_file, 'r') as f:
                content = f.read()
            
            # æå–å¼ é‡å½¢çŠ¶
            import re
            tensor_pattern = r'tensor<([^>]+)>'
            matches = re.findall(tensor_pattern, content)
            
            shapes = []
            for match in matches:
                if 'x' in match:
                    shape = [int(x) for x in match.split('x') if x.isdigit()]
                    shapes.append(shape)
            
            # ç¡®å®šæ“ä½œç±»å‹
            operation = "unknown"
            if "linalg.add" in content or "arith.addf" in content:
                operation = "add"
            elif "linalg.mul" in content or "arith.mulf" in content:
                operation = "mul"
            elif "linalg.sub" in content or "arith.subf" in content:
                operation = "sub"
            elif "linalg.div" in content or "arith.divf" in content:
                operation = "div"
            elif "math.tanh" in content:
                operation = "tanh"
            elif "math.sin" in content:
                operation = "sin"
            elif "math.cos" in content:
                operation = "cos"
            elif "math.exp" in content:
                operation = "exp"
            elif "math.log" in content:
                operation = "log"
            elif "math.sqrt" in content:
                operation = "sqrt"
            elif "math.abs" in content:
                operation = "abs"
            elif "linalg.matmul" in content:
                operation = "matmul"
            elif "linalg.conv_2d_nhwc_hwcf" in content:
                operation = "conv2d_nhwc_hwcf"
            elif "linalg.pooling_nhwc_max" in content:
                operation = "maxpool2d"
            elif "linalg.pooling_nhwc_avg" in content:
                operation = "avgpool2d"
            elif "math.sigmoid" in content:
                operation = "sigmoid"
            elif "arith.maximumf" in content and "arith.constant" in content:
                operation = "relu"
            
            # ç¡®å®šè¾“å…¥è¾“å‡ºå½¢çŠ¶
            if len(shapes) >= 2:
                input_shapes = shapes[:-1]
                output_shape = shapes[-1]
            else:
                input_shapes = [shapes[0]] if shapes else [[1, 8, 8, 8]]
                output_shape = shapes[0] if shapes else [1, 8, 8, 8]
            
            return input_shapes, output_shape, operation
            
        except Exception as e:
            print(f"Warning: Could not parse {mlir_file}: {e}")
            return [[1, 8, 8, 8]], [1, 8, 8, 8], "unknown"
    
    def test_single_operation(self, operation_name: str, mlir_file: str) -> Dict:
        """æµ‹è¯•å•ä¸ªæ“ä½œ"""
        print(f"  æµ‹è¯•æ“ä½œ: {operation_name}")
        
        result = {
            "operation": operation_name,
            "mlir_file": mlir_file,
            "input_data": [],
            "output_data": None,
            "input_shapes": [],
            "output_shape": None,
            "operation_type": "unknown",
            "success": True
        }
        
        try:
            # è§£æMLIRæ–‡ä»¶
            input_shapes, output_shape, op_type = self.parse_mlir_file(mlir_file)
            result["input_shapes"] = input_shapes
            result["output_shape"] = output_shape
            result["operation_type"] = op_type
            
            # ç”Ÿæˆè¾“å…¥æ•°æ®
            input_data = []
            for shape in input_shapes:
                data = self.generate_test_data(shape)
                input_data.append(data)
                result["input_data"].append(data.tolist())
            
            # æ¨¡æ‹Ÿæ“ä½œè¾“å‡º
            output_data = self.simulate_operation_output(op_type, input_data)
            result["output_data"] = output_data.tolist()
            
        except Exception as e:
            result["success"] = False
            result["error"] = str(e)
            print(f"    âŒ {operation_name}: {e}")
        
        return result
    
    def test_all_single_operations(self, sample_size: int = None):
        """æµ‹è¯•æ‰€æœ‰å•ä¸ªæ“ä½œ"""
        print("ğŸ”§ æµ‹è¯•æ‰€æœ‰å•ä¸ªæ“ä½œ...")
        
        single_ops = []
        if os.path.exists("out/single"):
            single_ops = [d for d in os.listdir("out/single") 
                         if os.path.isdir(os.path.join("out/single", d))]
        
        if sample_size:
            import random
            single_ops = random.sample(single_ops, min(sample_size, len(single_ops)))
        
        for op in single_ops:
            mlir_file = f"out/single/{op}/{op}_N1_H8_W8_C8.mlir"
            
            if os.path.exists(mlir_file):
                result = self.test_single_operation(op, mlir_file)
                self.dataset["single_operations"][op] = result
            else:
                print(f"    âš ï¸  {op}: MLIRæ–‡ä»¶ä¸å­˜åœ¨")
        
        self.dataset["metadata"]["single_operations"] = len(self.dataset["single_operations"])
    
    def test_all_pair_operations(self, sample_size: int = None):
        """æµ‹è¯•æ‰€æœ‰æˆå¯¹æ“ä½œ"""
        print("ğŸ”— æµ‹è¯•æ‰€æœ‰æˆå¯¹æ“ä½œ...")
        
        pair_ops = []
        if os.path.exists("out/pairs_complete"):
            pair_ops = [d for d in os.listdir("out/pairs_complete") 
                       if os.path.isdir(os.path.join("out/pairs_complete", d))]
        
        if sample_size:
            import random
            pair_ops = random.sample(pair_ops, min(sample_size, len(pair_ops)))
        
        for pair_name in pair_ops:
            print(f"  æµ‹è¯•æˆå¯¹æ“ä½œ: {pair_name}")
            
            # æŸ¥æ‰¾MLIRæ–‡ä»¶
            mlir_dir = f"out/pairs_complete/{pair_name}"
            mlir_files = []
            if os.path.exists(mlir_dir):
                for root, dirs, files in os.walk(mlir_dir):
                    mlir_files.extend([os.path.join(root, f) for f in files if f.endswith('.mlir')])
            
            if mlir_files:
                mlir_file = mlir_files[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ–‡ä»¶
                result = self.test_single_operation(pair_name, mlir_file)
                result["pair_type"] = "sequential" if "_then_" in pair_name else "combined"
                result["mlir_files"] = mlir_files
                
                self.dataset["pair_operations"][pair_name] = result
            else:
                print(f"    âš ï¸  {pair_name}: æœªæ‰¾åˆ°MLIRæ–‡ä»¶")
        
        self.dataset["metadata"]["pair_operations"] = len(self.dataset["pair_operations"])
    
    def generate_statistics(self):
        """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        print("ğŸ“Š ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯...")
        
        stats = {
            "total_operations": len(self.dataset["single_operations"]) + len(self.dataset["pair_operations"]),
            "successful_operations": 0,
            "operation_types": {},
            "shape_distribution": {},
            "data_ranges": {}
        }
        
        # ç»Ÿè®¡å•ä¸ªæ“ä½œ
        for op, result in self.dataset["single_operations"].items():
            if result["success"]:
                stats["successful_operations"] += 1
                
                # ç»Ÿè®¡æ“ä½œç±»å‹
                op_type = result["operation_type"]
                stats["operation_types"][op_type] = stats["operation_types"].get(op_type, 0) + 1
                
                # ç»Ÿè®¡å½¢çŠ¶åˆ†å¸ƒ
                for shape in result["input_shapes"]:
                    shape_key = "x".join(map(str, shape))
                    stats["shape_distribution"][shape_key] = stats["shape_distribution"].get(shape_key, 0) + 1
        
        # ç»Ÿè®¡æˆå¯¹æ“ä½œ
        for op, result in self.dataset["pair_operations"].items():
            if result["success"]:
                stats["successful_operations"] += 1
                
                # ç»Ÿè®¡æ“ä½œç±»å‹
                op_type = result["operation_type"]
                stats["operation_types"][op_type] = stats["operation_types"].get(op_type, 0) + 1
        
        self.dataset["statistics"] = stats
    
    def save_dataset(self):
        """ä¿å­˜æ•°æ®é›†"""
        print("ğŸ’¾ ä¿å­˜æ•°æ®é›†...")
        
        # ä¿å­˜å®Œæ•´æ•°æ®é›†
        dataset_file = self.output_dir / "io_dataset_simulated.json"
        with open(dataset_file, 'w', encoding='utf-8') as f:
            json.dump(self.dataset, f, indent=2, ensure_ascii=False)
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        self.generate_markdown_report()
        
        print(f"âœ… æ•°æ®é›†å·²ä¿å­˜: {dataset_file}")
    
    def generate_markdown_report(self):
        """ç”ŸæˆMarkdownæŠ¥å‘Š"""
        report_file = self.output_dir / "io_dataset_simulated_report.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# è¾“å…¥-è¾“å‡º-IRæ•°æ®é›†æŠ¥å‘Šï¼ˆæ¨¡æ‹Ÿï¼‰\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {self.dataset['metadata']['generated_at']}\n")
            f.write(f"**æµ‹è¯•ç±»å‹**: {self.dataset['metadata']['test_type']}\n")
            f.write(f"**æ€»æ“ä½œæ•°**: {self.dataset['metadata']['total_operations']}\n")
            f.write(f"**å•ä¸ªæ“ä½œæ•°**: {self.dataset['metadata']['single_operations']}\n")
            f.write(f"**æˆå¯¹æ“ä½œæ•°**: {self.dataset['metadata']['pair_operations']}\n\n")
            
            # ç»Ÿè®¡ä¿¡æ¯
            stats = self.dataset["statistics"]
            f.write("## ğŸ“Š ç»Ÿè®¡ä¿¡æ¯\n\n")
            f.write(f"- **æ€»æ“ä½œæ•°**: {stats['total_operations']}\n")
            f.write(f"- **æˆåŠŸæ“ä½œæ•°**: {stats['successful_operations']}\n")
            f.write(f"- **æˆåŠŸç‡**: {stats['successful_operations']/max(1, stats['total_operations'])*100:.1f}%\n\n")
            
            # æ“ä½œç±»å‹åˆ†å¸ƒ
            f.write("## ğŸ”§ æ“ä½œç±»å‹åˆ†å¸ƒ\n\n")
            f.write("| æ“ä½œç±»å‹ | æ•°é‡ |\n")
            f.write("|----------|------|\n")
            for op_type, count in sorted(stats["operation_types"].items()):
                f.write(f"| {op_type} | {count} |\n")
            
            # å½¢çŠ¶åˆ†å¸ƒ
            f.write("\n## ğŸ“ è¾“å…¥å½¢çŠ¶åˆ†å¸ƒ\n\n")
            f.write("| å½¢çŠ¶ | æ•°é‡ |\n")
            f.write("|------|------|\n")
            for shape, count in sorted(stats["shape_distribution"].items()):
                f.write(f"| {shape} | {count} |\n")
            
            # æˆåŠŸæ“ä½œåˆ—è¡¨
            f.write("\n## âœ… æˆåŠŸæ“ä½œåˆ—è¡¨\n\n")
            f.write("### å•ä¸ªæ“ä½œ\n\n")
            for op, result in self.dataset["single_operations"].items():
                if result["success"]:
                    f.write(f"- **{op}** ({result['operation_type']}): {result['input_shapes']} â†’ {result['output_shape']}\n")
            
            f.write("\n### æˆå¯¹æ“ä½œ\n\n")
            for op, result in self.dataset["pair_operations"].items():
                if result["success"]:
                    f.write(f"- **{op}** ({result['operation_type']}): {result['input_shapes']} â†’ {result['output_shape']}\n")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹ç”Ÿæˆæ¨¡æ‹Ÿè¾“å…¥-è¾“å‡º-IRæ•°æ®é›†...")
    
    test = SimpleIOTest()
    
    # æµ‹è¯•æ‰€æœ‰å•ä¸ªæ“ä½œ
    test.test_all_single_operations()
    
    # æµ‹è¯•æ‰€æœ‰æˆå¯¹æ“ä½œ
    test.test_all_pair_operations()
    
    # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
    test.generate_statistics()
    
    # ä¿å­˜æ•°æ®é›†
    test.save_dataset()
    
    print("âœ… æ¨¡æ‹Ÿè¾“å…¥-è¾“å‡º-IRæ•°æ®é›†ç”Ÿæˆå®Œæˆï¼")
    print(f"ğŸ“Š æ€»æ“ä½œæ•°: {test.dataset['metadata']['total_operations']}")
    print(f"âœ… æˆåŠŸæ“ä½œ: {test.dataset['statistics']['successful_operations']}")

if __name__ == "__main__":
    main()
